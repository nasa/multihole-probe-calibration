{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Hole probe Calibration using ML\n",
    "This is an example for developing a ML model for 5 Hole NASA Probe. It's not as well documented as the Aerodyne probe which comes with a presentation in addition to the CSV Files. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why use Machine Learning\n",
    "5 Hole probes contain readings from 5 different pressure transducers. The calibration procedure involves testing them against a jet where the probe head is rotated in the pitch and yaw directions. \n",
    "\n",
    "This produces lots of data:\n",
    "Pressure Readings \n",
    "- Hole 1 (P1, FAP1)\n",
    "- Hole 2 (P2,FAP2)\n",
    "- Hole 3 (P3,FAP3)\n",
    "- Hole 4 (P4,FAP4)\n",
    "- Hole 5 (P5,FAP5)\n",
    "- Ambient/Static (Pa, FAP6)\n",
    "\n",
    "Other Data \n",
    "- Yaw and Pitch from calibration machine. \n",
    "- Mach Number: function of P0/P \n",
    "\n",
    "<div>\n",
    "<img src=\"images/5-hole probe.jpg\" style=\"height:300px\" />\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Data\n",
    "The data is stored in a csv file. The headers we are interested in are **FAP2-FAP6**. *This is what we are training on.*\n",
    "\n",
    "What we are predicted are **YAW**, **PITCH**, **MFJ**, **PTFJ**\n",
    "\n",
    "> **_NOTE:_**  The data are in pressures which have units. Machine learning optimizes coefficients in a matrix. Data that isn't normalized can create illconditioned matrices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing the Data\n",
    "Nondimensioning the data to be 0 and 1 helps the training process converge much quicker. 4 nondimensional coefficients were created from the dataset. \n",
    "FA\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/normalization_mhp.png\" style=\"width:600px\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placing Functions into Memory\n",
    "The code to normalize the data is shown below. Here's a brief description of each of the code blocks.\n",
    "\n",
    "1. **Import headers**: All external codes that are required.\n",
    "2. **Create Coefficients**: Create the coefficients \n",
    "3. **Create Data Loaders**: Required to load data in batch format to pytorch model for training\n",
    "4. **TrainMultilayer**: Code to train and save the MultiLayer Network\n",
    "5. **TrainKanNetwork**: Code to train and save the KAN Network \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/nasa/multihole-probe-calibration.git\n",
    "!cp -r multihole-probe-calibration/* .\n",
    "!rm -r multihole-probe-calibration\n",
    "!cp -r tutorials/5HoleProbe ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from multihole_probe import NeuralNet_MultiLayer, KAN\n",
    "from multihole_probe.train_model import train_model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateCoefficients(df:pd.DataFrame):\n",
    "    \"\"\"Create the coefficients for model 1 and model 2\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): dataframe containing probe data\n",
    "    \"\"\"\n",
    "    FAP2 = df['FAP2'].values\t# Hole 5, Center Hole \n",
    "    FAP3 = df['FAP3'].values\t# Hole 1\n",
    "    FAP4 = df['FAP4'].values    # Hole 2\n",
    "    FAP5 = df['FAP5'].values    # Hole 3\n",
    "    FAP6 = df['FAP6'].values    # Hole 4\n",
    "    Pt = df['PTFJ'].values\n",
    "    Ps = df['PSFJ'].values\n",
    "    # Predicted \n",
    "    theta = df['YAW'].values \t# Model 1\n",
    "    phi = df['PITCH'].values \t# Model 1\n",
    "    mach = df['MFJ'].values\t\t# Model 2\n",
    "    Cpt = (FAP2-Ps) / (Pt-Ps) \t# Model 2\n",
    "    # Used to train model to predicted values of Theta, Phi, Mach, Cpt\n",
    "    Pavg = 0.25 * (FAP3 + FAP4 + FAP5 + FAP6)\n",
    "    Cp1 = (FAP3-Pavg)/(FAP2-Pavg) \n",
    "    Cp2 = (FAP4-Pavg)/(FAP2-Pavg)\n",
    "    Cp3 = (FAP5-Pavg)/(FAP2-Pavg)\n",
    "    Cp4 = (FAP6-Pavg)/(FAP2-Pavg)\n",
    "    Cpm = (FAP2-Pavg)/(FAP2)\n",
    " \n",
    "    data = np.stack([Cp1,Cp2,Cp3,Cp4,Cpm,theta,phi]).transpose()\n",
    "    model1_data = pd.DataFrame(data=data, index=np.arange(0,len(Cp1)), columns=['Cp1','Cp2','Cp3','Cp4','Cpm','theta','phi'])\n",
    "    \n",
    "    data = np.stack([Cp1,Cp2,Cp3,Cp4,Cpm,mach,Cpt]).transpose()\n",
    "    model2_data = pd.DataFrame(data=data, index=np.arange(0,len(Cp1)), columns=['Cp1','Cp2','Cp3','Cp4','Cpm','mach','Cpt'])\n",
    "\n",
    "    return model1_data, model2_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateDataLoaders(in_features:int):\n",
    "    \"\"\"Read in the data and create the torch data loaders \n",
    "\n",
    "    Args:\n",
    "        in_features (int): number of input features \n",
    "\n",
    "    Returns:\n",
    "        (Tuple) containing:\n",
    "        \n",
    "            *model1_train_loader* (DataLoader): DataLoader to predict Theta and Phi\n",
    "            *model1_test_loader* (DataLoader): DataLoader to validate Theta and Phi\n",
    "            *model2_train_loader* (DataLoader): DataLoader to predict Mach and Cpt\n",
    "            *model2_test_loader* (DataLoader): DataLoader to validate Mach and Cpt\n",
    "            \n",
    "    \"\"\"\n",
    "    data = pickle.load(open('checkpoints/01_5HoleProbe_data.pkl', 'rb'))\n",
    "    \n",
    "    # Multi-layer perception \n",
    "    \n",
    "    model1_x_train = data['model1']['train'].to_numpy()[:,:in_features] \n",
    "    model1_y_train = data['model1']['train'].to_numpy()[:,in_features:]\n",
    "    model1_x_test = data['model1']['test'].to_numpy()[:,:in_features] \n",
    "    model1_y_test = data['model1']['test'].to_numpy()[:,in_features:]\n",
    "    \n",
    "    model2_x_train = data['model2']['train'].to_numpy()[:,:in_features] \n",
    "    model2_y_train = data['model2']['train'].to_numpy()[:,in_features:]\n",
    "    model2_x_test = data['model2']['test'].to_numpy()[:,:in_features] \n",
    "    model2_y_test = data['model2']['test'].to_numpy()[:,in_features:]\n",
    "    \n",
    "    model1_train_dataset = torch.utils.data.TensorDataset(torch.tensor(model1_x_train).float(),torch.tensor(model1_y_train).float())\n",
    "    model1_test_dataset = torch.utils.data.TensorDataset(torch.tensor(model1_x_test).float(),torch.tensor(model1_y_test).float())\n",
    "    \n",
    "    model2_train_dataset = torch.utils.data.TensorDataset(torch.tensor(model2_x_train).float(),torch.tensor(model2_y_train).float())\n",
    "    model2_test_dataset = torch.utils.data.TensorDataset(torch.tensor(model2_x_test).float(),torch.tensor(model2_y_test).float())\n",
    "    \n",
    "    model1_train_loader = torch.utils.data.DataLoader(dataset=model1_train_dataset,batch_size=64)\n",
    "    model1_test_loader = torch.utils.data.DataLoader(dataset=model1_test_dataset,batch_size=64)\n",
    "    \n",
    "    model2_train_loader = torch.utils.data.DataLoader(dataset=model2_train_dataset,batch_size=64)\n",
    "    model2_test_loader = torch.utils.data.DataLoader(dataset=model2_test_dataset,batch_size=64)\n",
    "    return model1_train_loader, model1_test_loader, model2_train_loader, model2_test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainMultiLayer(in_features:int,out_features:int,hidden_layers:int):\n",
    "    \"\"\"Train a standard DNN Network \n",
    "\n",
    "    Args:\n",
    "        in_features (int): Number of inputs\n",
    "        out_features (int): Number of outputs \n",
    "        hidden_layers (int): Size of hidden layers. Example, [8,8,8,8] 4 layers with 8 neurons per layer \n",
    "\n",
    "\n",
    "    Returns:\n",
    "        Dict: Dictionary containing the model states and training and validation loss history\n",
    "        \n",
    "    \"\"\"\n",
    "    nn_model1 = NeuralNet_MultiLayer(in_features,hidden_layers,out_features)\n",
    "    nn_model2 = NeuralNet_MultiLayer(in_features,hidden_layers,out_features)\n",
    "    \n",
    "    model1_train_loader, model1_test_loader, model2_train_loader, model2_test_loader = CreateDataLoaders(in_features)\n",
    "    \n",
    "    nn_model1,training_loss_history1,validation_loss_history1 = train_model(nn_model1,model1_train_loader,model1_test_loader,num_epochs=2000,verbose=True)\n",
    "    nn_model2,training_loss_history2,validation_loss_history2 = train_model(nn_model2,model2_train_loader,model2_test_loader,num_epochs=2000,verbose=True)\n",
    "    \n",
    "    return {'nn_model1': nn_model1.state_dict(),\n",
    "            'nn_training_loss_history1':training_loss_history1,\n",
    "            'nn_validation_loss_history1':validation_loss_history1,\n",
    "            'nn_model2': nn_model2.state_dict(),\n",
    "            'nn_training_loss_history2':training_loss_history2,\n",
    "            'nn_validation_loss_history2':validation_loss_history2,\n",
    "            'in_features':in_features,\n",
    "            'out_features':out_features,\n",
    "            'hidden_layers':hidden_layers\n",
    "            }\n",
    "            \n",
    "\n",
    "def TrainKanNetwork(in_features:int,out_features:int,hidden_layers:int):\n",
    "    \"\"\"Train the KAN Network\n",
    "    \n",
    "    https://github.com/Blealtan/efficient-kan/tree/master\n",
    "    \n",
    "    Args:\n",
    "        in_features (int): Number of inputs\n",
    "        out_features (int): Number of outputs \n",
    "        hidden_layers (int): Size of hidden layers. Example, [8,8,8,8] 4 layers with 8 neurons per layer \n",
    "\n",
    "    Returns:\n",
    "        Dict: Dictionary containing the model states and training and validation loss history\n",
    "        \n",
    "    \"\"\"\n",
    "    kan_layers = [in_features]; kan_layers.extend(hidden_layers); kan_layers.append(out_features)\n",
    "    kan_model1 = KAN(kan_layers)\n",
    "    kan_model2 = KAN(kan_layers)\n",
    "    \n",
    "    model1_train_loader, model1_test_loader, model2_train_loader, model2_test_loader = CreateDataLoaders(in_features)\n",
    "    \n",
    "    kan_model1,training_loss_history1,validation_loss_history1 = train_model(kan_model1,model1_train_loader,model1_test_loader,num_epochs=500,verbose=True)\n",
    "    kan_model2,training_loss_history2,validation_loss_history2 = train_model(kan_model2,model2_train_loader,model2_test_loader,num_epochs=500,verbose=True)  \n",
    "    \n",
    "    \n",
    "    return {'KAN_model1': kan_model1.state_dict(),\n",
    "            'KAN_training_loss_history1':training_loss_history1,\n",
    "            'KAN_validation_loss_history1':validation_loss_history1,\n",
    "            'KAN_model2': kan_model2.state_dict(),\n",
    "            'KAN_training_loss_history2':training_loss_history2,\n",
    "            'KAN_validation_loss_history2':validation_loss_history2\n",
    "            }\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 Processing the Data \n",
    "Now that we have placed all the functions into memory it's time to run the code to process the data. For this example, we are using Probe 1. The 2 code blocks below create a file in the checkpoint folder. This file contains the dataloaders along with the minmax scalers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Step1_ProcessData():\n",
    "    \"\"\"Process the data into test and train for both models\n",
    "    \"\"\"\n",
    "    # Specifies what data points to use based on calibration data. \n",
    "    # Probe caller specifies the probe. Total 10 probes\n",
    "    probe_caller = 1\n",
    "    if probe_caller   == 1:\n",
    "        probe_loc = [0,1805] # Reading numbers corresponding to different Probes \n",
    "    elif probe_caller == 2:\n",
    "        probe_loc = [1806,3609] \n",
    "    elif probe_caller == 3:\n",
    "        probe_loc = [3610,5414]\n",
    "    elif probe_caller == 4:\n",
    "        probe_loc = [5415,7219]\n",
    "    elif probe_caller == 5:\n",
    "        probe_loc = [7220,9026]\n",
    "    elif probe_caller == 6:\n",
    "        probe_loc = [9027,10831]\n",
    "    elif probe_caller == 7:\n",
    "        probe_loc = [10832,12641]\n",
    "    elif probe_caller == 8:\n",
    "        probe_loc = [12642,14447]\n",
    "    elif probe_caller == 9:\n",
    "        probe_loc = [14448,16252] \n",
    "    elif probe_caller == 10:\n",
    "        probe_loc = [16253,18061]\n",
    "        \n",
    "    #%% Read in the data \n",
    "    df = pd.read_csv('dataset/5Hole-Probe1.csv')\n",
    "    # Select from datapoints \n",
    "    probe_1 = df.iloc[probe_loc[0]:probe_loc[1]]\n",
    "    # validation_set = probe_1.query('MFJ<0.55 and MFJ>0.45')\n",
    "    # test_train_set = probe_1.query('MFJ>0.55 or MFJ<0.45')\n",
    "\n",
    "    model1_data,model2_data = CreateCoefficients(df=probe_1)\n",
    "    train1,test1 = train_test_split(model1_data,test_size=0.2)\n",
    "    train2,test2 = train_test_split(model2_data,test_size=0.2)\n",
    "\n",
    "    minmax_model1 = MinMaxScaler(feature_range=(0, 1))\n",
    "    minmax_model2 = MinMaxScaler(feature_range=(0, 1))\n",
    "    minmax_model1.fit(model1_data)\n",
    "    minmax_model2.fit(model2_data)\n",
    "    \n",
    "    os.makedirs('checkpoints',exist_ok=True)\n",
    "    os.makedirs('results',exist_ok=True)\n",
    "    \n",
    "    pickle.dump({\n",
    "                    'model1':\n",
    "                        {\n",
    "                            'data':model1_data, \n",
    "                            'train':train1,\n",
    "                            'test':test1,\n",
    "                            'minmax':minmax_model1\n",
    "                        },\n",
    "                    'model2':\n",
    "                        {\n",
    "                            'data':model2_data,\n",
    "                            'train':train2,\n",
    "                            'test':test2,\n",
    "                            'minmax':minmax_model2\n",
    "                        }\n",
    "                },\n",
    "                open('checkpoints/01_5HoleProbe_data.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Step1_ProcessData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Models\n",
    "The code below trains the Neural Network and KAN Models. Try changing the hidden_layer size to see if accuracy could be improved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Step2_TrainModels():\n",
    "    \"\"\"Build the models and train them \n",
    "    \"\"\"\n",
    "    in_features = 5; out_features = 2 # For both model 1 and 2\n",
    "    hidden_layers = [64,64,64,64]\n",
    "    nn_data = TrainMultiLayer(in_features,out_features,hidden_layers)\n",
    "    kan_data = TrainKanNetwork(in_features,out_features,hidden_layers)\n",
    "\n",
    "    pickle.dump(dict(nn_data, **kan_data),\n",
    "                open('checkpoints/02_5HoleProbe_Trained_Models.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Step2_TrainModels()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference \n",
    "Inference is used to read in the machine learning model and report the output. Think of it as a query or user interface to the machine learning model. \n",
    "\n",
    "The user will supply the Pressures from the probe read out FAP1-5 for all the holes with FAP1 being the first hole. The inference code will take this data and compute all the `Cp` coefficients and plug into the ML model 1 and model 2. Model 1 predicts the `theta` and `phi`. Model 2 predicts `Mach` and `Cpt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "from typing import Union\n",
    "sys.path.insert(0,'../../')\n",
    "from multihole_probe import NeuralNet_MultiLayer, KAN\n",
    "import numpy.typing as npt\n",
    "import numpy as np \n",
    "import torch \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Code: `predict_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_data(FAP1:Union[float,npt.NDArray],FAP2:Union[float,npt.NDArray],\n",
    "                 FAP3:Union[float,npt.NDArray],FAP4:Union[float,npt.NDArray],\n",
    "                 FAP5:Union[float,npt.NDArray], Ps:Union[float,npt.NDArray]):\n",
    "    \"\"\"Predicts the data \n",
    "\n",
    "    Args:\n",
    "        FAP1 (Union[float,npt.NDArray]): Hole 1, Center Hole\n",
    "        FAP2 (Union[float,npt.NDArray]): Hole 2\n",
    "        FAP3 (Union[float,npt.NDArray]): Hole 3\n",
    "        FAP4 (Union[float,npt.NDArray]): Hole 4\n",
    "        FAP5 (Union[float,npt.NDArray]): Hole 5\n",
    "        Ps (Union[float,npt.NDArray]): Static Pressure\n",
    "    \"\"\"\n",
    "    # Process the Data \n",
    "    data = pickle.load(open('checkpoints/01_5HoleProbe_data.pkl','rb'))\n",
    "    model1_minmax = data['model1']['minmax']\n",
    "    model2_minmax = data['model2']['minmax']\n",
    "    Pavg = 0.25 * (FAP2 + FAP3 + FAP4 + FAP5)\n",
    "    Cp1 = (FAP2-Pavg)/(FAP1-Pavg) \n",
    "    Cp2 = (FAP3-Pavg)/(FAP1-Pavg)\n",
    "    Cp3 = (FAP4-Pavg)/(FAP1-Pavg)\n",
    "    Cp4 = (FAP5-Pavg)/(FAP1-Pavg)\n",
    "    Cpm = (FAP1-Pavg)/(FAP1)\n",
    "    \n",
    "    input = torch.tensor(np.stack([Cp1,Cp2,Cp3,Cp4,Cpm,Cp1*0,Cp1*0]).transpose())\n",
    "    model1_minmax.fit(input)\n",
    "    input = input.float()\n",
    "    \n",
    "    # Build the model \n",
    "    model_data = pickle.load(open('checkpoints/02_5HoleProbe_Trained_Models.pkl','rb'))\n",
    "\n",
    "    nn_model1 = NeuralNet_MultiLayer(model_data['in_features'],\n",
    "                        model_data['hidden_layers'],\n",
    "                        model_data['out_features'])\n",
    "    nn_model2 = NeuralNet_MultiLayer(model_data['in_features'],\n",
    "                        model_data['hidden_layers'],\n",
    "                        model_data['out_features'])\n",
    "    \n",
    "    kan_layers = [model_data['in_features']]; kan_layers.extend(model_data['hidden_layers']); kan_layers.append(model_data['out_features'])\n",
    "    kan_model1 = KAN(kan_layers)\n",
    "    kan_model2 = KAN(kan_layers)\n",
    "    \n",
    "    nn_model1.load_state_dict(model_data['nn_model1'])\n",
    "    nn_model2.load_state_dict(model_data['nn_model2'])\n",
    "    kan_model1.load_state_dict(model_data['KAN_model1'])\n",
    "    kan_model2.load_state_dict(model_data['KAN_model2'])\n",
    "    \n",
    "    # Predict the Data \n",
    "    nn_out1 = nn_model1(input[:,:model_data['in_features']])\n",
    "    nn_out2 = nn_model2(input[:,:model_data['in_features']])\n",
    "    kan_out1 = kan_model1(input[:,:model_data['in_features']])\n",
    "    kan_out2 = kan_model2(input[:,:model_data['in_features']])\n",
    "    \n",
    "    # Scale data back \n",
    "    input[:,-2:] = nn_out1\n",
    "    nn_out1 = model1_minmax.inverse_transform(input.detach().numpy())\n",
    "    input[:,-2:] = nn_out2\n",
    "    nn_out2 = model2_minmax.inverse_transform(input.detach().numpy())\n",
    "    \n",
    "    input[:,-2:] = kan_out1\n",
    "    kan_out1 = model1_minmax.inverse_transform(input.detach().numpy())\n",
    "    input[:,-2:] = kan_out2\n",
    "    kan_out2 = model2_minmax.inverse_transform(input.detach().numpy())\n",
    "    \n",
    "    return nn_out1,nn_out2,kan_out1,kan_out2\n",
    "    \n",
    "def plot(out1:npt.NDArray,out2:npt.NDArray,network_name:str):\n",
    "    \"\"\"Plot the errors\n",
    "\n",
    "    Args:\n",
    "        out1 (npt.NDArray): Output from model 1\n",
    "        out2 (npt.NDArray): Output from model 2\n",
    "        model_name (str): Name of the neural network \n",
    "    \"\"\"\n",
    "    df = pd.read_csv('dataset/5Hole-Probe1.csv')\n",
    "    probe_loc = [0,1805]\n",
    "    df = df.iloc[probe_loc[0]:probe_loc[1]]\n",
    "    FAP2 = df['FAP2'].values\t# Hole 5, Center Hole \n",
    "    Ps = df['PSFJ'].values\n",
    "    theta = df['YAW'].values\n",
    "    phi = df['PITCH'].values\n",
    "    mach = df['MFJ'].values\n",
    "    Pt = df['PTFJ'].values\n",
    "    \n",
    "    Pt_predict = (FAP2-Ps) / out2[:,-1] + Ps\n",
    "    # Plot Relative Percent difference\n",
    "    # https://stats.stackexchange.com/questions/86708/how-to-calculate-relative-error-when-the-true-value-is-zero\n",
    "    theta_err = 2*(theta - out1[:,-2])/(theta+out1[:,-2])\n",
    "    phi_err = 2*(phi - out1[:,-1])/(phi + out1[:,-1])\n",
    "    \n",
    "    mach_err = (mach - out2[:,-2])/mach\n",
    "    Pt_err = (Pt - Pt_predict)/Pt\n",
    "    \n",
    "    # Plot of Error \n",
    "    plt.figure(num=1,clear=True,figsize=(10,8))\n",
    "    plt.tricontourf(theta,phi,theta_err,levels=11)\n",
    "    plt.xlabel('Theta')\n",
    "    plt.ylabel('Phi')\n",
    "    plt.title(f'{network_name} Theta Relative Percent Difference Error')\n",
    "    plt.colorbar()\n",
    "    plt.rcParams.update({'font.size': 18})\n",
    "    plt.savefig(f'results/{network_name}_theta_error.png',dpi=300,transparent=None)\n",
    "\n",
    "    plt.figure(num=2,clear=True,figsize=(10,8))\n",
    "    plt.tricontourf(theta,phi,phi_err,levels=11)\n",
    "    plt.xlabel('Theta')\n",
    "    plt.ylabel('Phi')\n",
    "    plt.title(f'{network_name} Phi Relative Percent Difference Error')\n",
    "    plt.colorbar()\n",
    "    plt.rcParams.update({'font.size': 18})\n",
    "    plt.savefig(f'results/{network_name}_Phi_error.png',dpi=300,transparent=None)\n",
    "\n",
    "    plt.figure(num=3,clear=True,figsize=(10,8))\n",
    "    plt.tricontourf(theta,phi,mach_err,levels=11)\n",
    "    plt.xlabel('Theta')\n",
    "    plt.ylabel('Phi')\n",
    "    plt.title(f'{network_name} Mach Percent Error')\n",
    "    plt.colorbar()\n",
    "    plt.rcParams.update({'font.size': 18})\n",
    "    plt.savefig(f'results/{network_name}_Mach_error.png',dpi=300,transparent=None)\n",
    "    \n",
    "    plt.figure(num=4,clear=True,figsize=(10,8))\n",
    "    plt.tricontourf(theta,phi,Pt_err,levels=11)\n",
    "    plt.xlabel('Theta')\n",
    "    plt.ylabel('Phi')\n",
    "    plt.title(f'{network_name} Pt Percent Error')\n",
    "    plt.colorbar()\n",
    "    plt.rcParams.update({'font.size': 18})\n",
    "    plt.savefig(f'results/{network_name}_Pt_error.png',dpi=300,transparent=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Inference\n",
    "The code below is an example of how to run the inference code with a set of values followed by plotting of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset/5Hole-Probe1.csv')\n",
    "probe_loc = [0,1805]\n",
    "df = df.iloc[probe_loc[0]:probe_loc[1]]\n",
    "FAP2 = df['FAP2'].values\t# Hole 5, Center Hole \n",
    "FAP3 = df['FAP3'].values\t# Hole 1\n",
    "FAP4 = df['FAP4'].values    # Hole 2\n",
    "FAP5 = df['FAP5'].values    # Hole 3\n",
    "FAP6 = df['FAP6'].values    # Hole 4\n",
    "Ps = df['PSFJ'].values\n",
    "\n",
    "nn_out1,nn_out2,kan_out1,kan_out2 = predict_data(FAP2,FAP3,FAP4,FAP5,FAP6,Ps)\n",
    "\n",
    "plot(nn_out1,nn_out2,'Multi-Layer Perception')\n",
    "plot(kan_out1,kan_out2,'KAN')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
